#!/bin/bash
#PBS -P NKBert                 
#PBS -l select=1:ncpus=1:ngpus=1:mem=180GB 
#PBS -l walltime=60:00:00      
#PBS -j oe

module load cuda/10.2.89
module load python/3.7.7 magma/2.5.3 openmpi-gcc/3.1.5
python run_language_modeling.py --train_data_file=/project/RDS-FASS-NKBert-RW/data/tokenized.txt --line_by_line --output_dir /project/RDS-FASS-NKBert-RW/jobert --tokenizer_name /project/RDS-FASS-NKBert-RW/kobert --model_name_or_path /project/RDS-FASS-NKBert-RW/kobert --mlm --do_train --save_steps 100000 --learning_rate 1e-4 --num_train_epochs 1 --per_gpu_train_batch_size 8 --seed 42

